Results of model fitting 'randomForest and XGBoost':


Variable: sand.wfraction
Ranger result

Call:
 ranger(formula = fm.t, data = dfs, importance = "impurity", write.forest = TRUE,      mtry = t.mrfX$bestTune$mtry, num.trees = 85, case.weights = case.weights.s) 

Type:                             Regression 
Number of trees:                  85 
Sample size:                      1474546 
Number of independent variables:  243 
Mtry:                             115 
Target node size:                 5 
Variable importance mode:         impurity 
Splitrule:                        variance 
OOB prediction error (MSE):       73.28199 
R squared (OOB):                  0.9081825 

 Variable importance:
          [,1]
DEPTH 65207223
PC8   24559811
PC18  23718187
PC241 23685873
PC4   21134406
PC2   19384205
PC234 16864112
PC223 14181998
PC73  12490357
PC215 11220644
PC194 10667874
PC242 10656929
PC240 10315618
PC235 10103617
PC236  9756174
PC228  9446516
PC227  9216383
PC14   8350720
PC64   8074010
PC51   7970339
PC68   7832451
PC7    7772044
PC137  7625716
PC62   7393288
PC49   7175634
PC203  7111548
PC209  7109000
PC15   6996253
PC54   6805719
PC207  6687966
PC230  6606908
PC28   6143210
PC199  6006448
PC220  6000087
PC12   5988345

eXtreme Gradient Boosting 

1474546 samples
    243 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 983030, 983031, 983031 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE      Rsquared   MAE     
  0.3  2           50      23.83112  0.2934764  19.71526
  0.3  2          100      23.24164  0.3260895  19.08049
  0.3  2          150      22.85999  0.3477168  18.69075
  0.3  3           50      22.96844  0.3424394  18.81704
  0.3  3          100      22.25493  0.3824429  18.10085
  0.3  3          150      21.82167  0.4060935  17.66310
  0.3  4           50      22.14073  0.3893608  17.99468
  0.3  4          100      21.31023  0.4340632  17.16465
  0.3  4          150      20.74828  0.4637066  16.62833
  0.4  2           50      23.64969  0.3015429  19.48349
  0.4  2          100      23.04397  0.3363802  18.85447
  0.4  2          150      22.66284  0.3580197  18.46780
  0.4  3           50      22.75168  0.3534076  18.56067
  0.4  3          100      22.00422  0.3949412  17.80864
  0.4  3          150      21.54301  0.4200590  17.35247
  0.4  4           50      21.92807  0.3993205  17.73615
  0.4  4          100      21.07386  0.4451652  16.89220
  0.4  4          150      20.47463  0.4763853  16.33117
  0.5  2           50      23.52381  0.3077409  19.30329
  0.5  2          100      22.88459  0.3446993  18.65922
  0.5  2          150      22.50305  0.3663668  18.28036
  0.5  3           50      22.59148  0.3613342  18.36045
  0.5  3          100      21.83485  0.4033311  17.60142
  0.5  3          150      21.35716  0.4291068  17.12956
  0.5  4           50      21.80308  0.4051167  17.55050
  0.5  4          100      20.87127  0.4550388  16.66906
  0.5  4          150      20.25754  0.4866984  16.09159

Tuning parameter 'gamma' was held constant at a value of 0
Tuning
 parameter 'min_child_weight' was held constant at a value of 1
Tuning
 parameter 'subsample' was held constant at a value of 1
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nrounds = 150, max_depth = 4, eta =
 0.5, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.

 XGBoost variable importance:
    Feature        Gain        Cover   Frequency
 1:     PC8 0.078147563 0.0059274937 0.004041311
 2:   PC234 0.064543179 0.0167122755 0.009878761
 3:   PC241 0.057969656 0.0131018263 0.017512348
 4:    PC18 0.055463165 0.0088029705 0.009429726
 5:     PC4 0.032782866 0.0147073769 0.014818141
 6:   PC223 0.024240750 0.0118896773 0.005837449
 7:   PC194 0.020092284 0.0081399106 0.011225864
 8:    PC64 0.018051091 0.0015907517 0.002694207
 9:   PC242 0.018020073 0.0097522248 0.004490346
10:    PC28 0.017432635 0.0077368857 0.007633588
11:    PC51 0.016486132 0.0008127004 0.003143242
12:    PC68 0.013747625 0.0059146875 0.006286484
13:   PC137 0.013590637 0.0044758528 0.004490346
14:   PC205 0.013280648 0.0076252174 0.008082622
15:   PC134 0.013198462 0.0064930931 0.003143242
16:   PC228 0.012656334 0.0112345860 0.007633588
17:   PC162 0.012539239 0.0013489483 0.002245173
18:    PC14 0.012213527 0.0017850658 0.003143242
19:    PC49 0.011330889 0.0025380868 0.003592277
20:   PC236 0.011079544 0.0017677192 0.003143242
21:    PC33 0.008976375 0.0091639892 0.006286484
22:   PC240 0.008920982 0.0101770835 0.014818141
23:   PC177 0.008533181 0.0020189775 0.002694207
24:   PC169 0.008415260 0.0103022702 0.005837449
25:   PC164 0.008355799 0.0075285932 0.004041311
    Feature        Gain        Cover   Frequency


 Support Vector Machine model:
SVM model on 243 features (cookie=1)
 Scenario: LS -1
 Formula:  sand_tot_psa ~ DEPTH + PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 +  
    PC8 + PC9 + PC10 + PC11 + PC12 + PC13 + PC14 + PC15 + PC16 +  
    PC17 + PC18 + PC19 + PC20 + PC21 + PC22 + PC23 + PC24 + PC25 +  
    PC26 + PC27 + PC28 + PC29 + PC30 + PC31 + PC32 + PC33 + PC34 +  
    PC35 + PC36 + PC37 + PC38 + PC39 + PC40 + PC41 + PC42 + PC43 +  
    PC44 + PC45 + PC46 + PC47 + PC48 + PC49 + PC50 + PC51 + PC52 +  
    PC53 + PC54 + PC55 + PC56 + PC57 + PC58 + PC59 + PC60 + PC61 +  
    PC62 + PC63 + PC64 + PC65 + PC66 + PC67 + PC68 + PC69 + PC70 +  
    PC71 + PC72 + PC73 + PC74 + PC75 + PC76 + PC77 + PC78 + PC79 +  
    PC80 + PC81 + PC82 + PC83 + PC84 + PC85 + PC86 + PC87 + PC88 +  
    PC89 + PC90 + PC91 + PC92 + PC93 + PC94 + PC95 + PC96 + PC97 +  
    PC98 + PC99 + PC100 + PC101 + PC102 + PC103 + PC104 + PC105 +  
    PC106 + PC107 + PC108 + PC109 + PC110 + PC111 + PC112 + PC113 +  
    PC114 + PC115 + PC116 + PC117 + PC118 + PC119 + PC120 + PC121 +  
    PC122 + PC123 + PC124 + PC125 + PC126 + PC127 + PC128 + PC129 +  
    PC130 + PC131 + PC132 + PC133 + PC134 + PC135 + PC136 + PC137 +  
    PC138 + PC139 + PC140 + PC141 + PC142 + PC143 + PC144 + PC145 +  
    PC146 + PC147 + PC148 + PC149 + PC150 + PC151 + PC152 + PC153 +  
    PC154 + PC155 + PC156 + PC157 + PC158 + PC159 + PC160 + PC161 +  
    PC162 + PC163 + PC164 + PC165 + PC166 + PC167 + PC168 + PC169 +  
    PC170 + PC171 + PC172 + PC173 + PC174 + PC175 + PC176 + PC177 +  
    PC178 + PC179 + PC180 + PC181 + PC182 + PC183 + PC184 + PC185 +  
    PC186 + PC187 + PC188 + PC189 + PC190 + PC191 + PC192 + PC193 +  
    PC194 + PC195 + PC196 + PC197 + PC198 + PC199 + PC200 + PC201 +  
    PC202 + PC203 + PC204 + PC205 + PC206 + PC207 + PC208 + PC209 +  
    PC210 + PC211 + PC212 + PC213 + PC214 + PC215 + PC216 + PC217 +  
    PC218 + PC219 + PC220 + PC221 + PC222 + PC223 + PC224 + PC225 +  
    PC226 + PC227 + PC228 + PC229 + PC230 + PC231 + PC232 + PC233 +  
    PC234 + PC235 + PC236 + PC237 + PC238 + PC239 + PC240 + PC241 +      PC242
  trained and selected on a 14x8208 grid

--------------------------------------
