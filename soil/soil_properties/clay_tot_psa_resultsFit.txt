Results of model fitting 'randomForest and XGBoost':


Variable: clay.wfraction
Ranger result

Call:
 ranger(formula = fm.t, data = dfs, importance = "impurity", write.forest = TRUE,      mtry = t.mrfX$bestTune$mtry, num.trees = 85, case.weights = case.weights.s) 

Type:                             Regression 
Number of trees:                  85 
Sample size:                      1474551 
Number of independent variables:  243 
Mtry:                             115 
Target node size:                 5 
Variable importance mode:         impurity 
Splitrule:                        variance 
OOB prediction error (MSE):       49.59072 
R squared (OOB):                  0.8596439 

 Variable importance:
          [,1]
DEPTH 39063177
PC234 18471786
PC240 11522391
PC215  7489264
PC241  7437001
PC242  5973881
PC231  5043258
PC228  4775995
PC222  4010274
PC199  3552319
PC28   3550704
PC227  3448576
PC1    3439463
PC170  3360587
PC46   3148054
PC5    3095210
PC177  3074110
PC232  3067008
PC51   2981646
PC137  2970518
PC214  2924249
PC4    2833937
PC194  2833033
PC15   2766522
PC122  2653850
PC47   2549083
PC233  2537603
PC7    2475206
PC3    2453773
PC41   2382738
PC13   2382324
PC164  2363891
PC33   2338570
PC16   2303383
PC40   2279732

eXtreme Gradient Boosting 

1474551 samples
    243 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 983033, 983034, 983035 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE      Rsquared   MAE     
  0.3  2           50      16.38277  0.2438065  12.80167
  0.3  2          100      16.06475  0.2720666  12.48395
  0.3  2          150      15.86881  0.2893853  12.29160
  0.3  3           50      15.92888  0.2851720  12.36273
  0.3  3          100      15.55047  0.3182498  11.99886
  0.3  3          150      15.29049  0.3409905  11.76311
  0.3  4           50      15.49469  0.3234783  11.96045
  0.3  4          100      15.01462  0.3648917  11.52572
  0.3  4          150      14.68151  0.3930334  11.23810
  0.4  2           50      16.29624  0.2503167  12.70163
  0.4  2          100      15.97343  0.2794561  12.38588
  0.4  2          150      15.76602  0.2978602  12.18700
  0.4  3           50      15.80430  0.2946931  12.22915
  0.4  3          100      15.40297  0.3300083  11.85117
  0.4  3          150      15.14434  0.3524862  11.62879
  0.4  4           50      15.34845  0.3351313  11.81305
  0.4  4          100      14.84131  0.3784557  11.36337
  0.4  4          150      14.48604  0.4082147  11.07037
  0.5  2           50      16.23573  0.2545581  12.62209
  0.5  2          100      15.89375  0.2857192  12.29260
  0.5  2          150      15.69937  0.3031230  12.11019
  0.5  3           50      15.73318  0.3002056  12.15160
  0.5  3          100      15.33021  0.3355930  11.78075
  0.5  3          150      15.04197  0.3604673  11.52414
  0.5  4           50      15.27845  0.3402702  11.73682
  0.5  4          100      14.74456  0.3855677  11.27574
  0.5  4          150      14.36927  0.4166342  10.96515

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'colsample_bytree'
 was held constant at a value of 0.8
Tuning parameter 'min_child_weight' was held constant at
 a value of 1
Tuning parameter 'subsample' was held constant at a value of 1
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nrounds = 150, max_depth = 4, eta = 0.5, gamma =
 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.

 XGBoost variable importance:
    Feature        Gain       Cover   Frequency
 1:   PC241 0.109016462 0.017659899 0.014071720
 2:   DEPTH 0.056117818 0.030261649 0.034498411
 3:   PC242 0.051249468 0.012675016 0.005901044
 4:   PC222 0.045568952 0.008441710 0.004085338
 5:   PC170 0.035311830 0.014839651 0.006354970
 6:   PC240 0.021257548 0.010065965 0.009078529
 7:    PC28 0.020250499 0.010789002 0.009986382
 8:   PC228 0.019191287 0.006881977 0.009078529
 9:   PC215 0.019179005 0.004031019 0.003177485
10:    PC83 0.017021858 0.006582812 0.007716750
11:    PC64 0.016611070 0.003162649 0.004085338
12:   PC232 0.016289579 0.009612033 0.008624603
13:   PC233 0.015955212 0.003441982 0.003177485
14:   PC117 0.014017111 0.006607577 0.005901044
15:   PC134 0.013817866 0.005736374 0.002723559
16:   PC194 0.013589847 0.003621069 0.002723559
17:     PC5 0.012019125 0.005415936 0.005901044
18:    PC47 0.010710808 0.003850659 0.004539265
19:    PC13 0.010370454 0.002646027 0.002723559
20:    PC68 0.010005006 0.008794413 0.004993191
21:   PC218 0.008540963 0.009297161 0.005901044
22:   PC220 0.008261954 0.004882883 0.004085338
23:   PC235 0.007924848 0.003458460 0.004085338
24:     PC4 0.007425760 0.010317951 0.010894235
25:    PC67 0.007302772 0.002113883 0.004085338
    Feature        Gain       Cover   Frequency


 Support Vector Machine model:
SVM model on 243 features (cookie=1)
 Scenario: LS -1
 Formula:  clay_tot_psa ~ DEPTH + PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 +  
    PC8 + PC9 + PC10 + PC11 + PC12 + PC13 + PC14 + PC15 + PC16 +  
    PC17 + PC18 + PC19 + PC20 + PC21 + PC22 + PC23 + PC24 + PC25 +  
    PC26 + PC27 + PC28 + PC29 + PC30 + PC31 + PC32 + PC33 + PC34 +  
    PC35 + PC36 + PC37 + PC38 + PC39 + PC40 + PC41 + PC42 + PC43 +  
    PC44 + PC45 + PC46 + PC47 + PC48 + PC49 + PC50 + PC51 + PC52 +  
    PC53 + PC54 + PC55 + PC56 + PC57 + PC58 + PC59 + PC60 + PC61 +  
    PC62 + PC63 + PC64 + PC65 + PC66 + PC67 + PC68 + PC69 + PC70 +  
    PC71 + PC72 + PC73 + PC74 + PC75 + PC76 + PC77 + PC78 + PC79 +  
    PC80 + PC81 + PC82 + PC83 + PC84 + PC85 + PC86 + PC87 + PC88 +  
    PC89 + PC90 + PC91 + PC92 + PC93 + PC94 + PC95 + PC96 + PC97 +  
    PC98 + PC99 + PC100 + PC101 + PC102 + PC103 + PC104 + PC105 +  
    PC106 + PC107 + PC108 + PC109 + PC110 + PC111 + PC112 + PC113 +  
    PC114 + PC115 + PC116 + PC117 + PC118 + PC119 + PC120 + PC121 +  
    PC122 + PC123 + PC124 + PC125 + PC126 + PC127 + PC128 + PC129 +  
    PC130 + PC131 + PC132 + PC133 + PC134 + PC135 + PC136 + PC137 +  
    PC138 + PC139 + PC140 + PC141 + PC142 + PC143 + PC144 + PC145 +  
    PC146 + PC147 + PC148 + PC149 + PC150 + PC151 + PC152 + PC153 +  
    PC154 + PC155 + PC156 + PC157 + PC158 + PC159 + PC160 + PC161 +  
    PC162 + PC163 + PC164 + PC165 + PC166 + PC167 + PC168 + PC169 +  
    PC170 + PC171 + PC172 + PC173 + PC174 + PC175 + PC176 + PC177 +  
    PC178 + PC179 + PC180 + PC181 + PC182 + PC183 + PC184 + PC185 +  
    PC186 + PC187 + PC188 + PC189 + PC190 + PC191 + PC192 + PC193 +  
    PC194 + PC195 + PC196 + PC197 + PC198 + PC199 + PC200 + PC201 +  
    PC202 + PC203 + PC204 + PC205 + PC206 + PC207 + PC208 + PC209 +  
    PC210 + PC211 + PC212 + PC213 + PC214 + PC215 + PC216 + PC217 +  
    PC218 + PC219 + PC220 + PC221 + PC222 + PC223 + PC224 + PC225 +  
    PC226 + PC227 + PC228 + PC229 + PC230 + PC231 + PC232 + PC233 +  
    PC234 + PC235 + PC236 + PC237 + PC238 + PC239 + PC240 + PC241 +      PC242
  trained and selected on a 14x8052 grid

--------------------------------------
