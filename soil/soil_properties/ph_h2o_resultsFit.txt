Results of model fitting 'randomForest and XGBoost':


Variable: ph.h2o
Ranger result

Call:
 ranger(formula = fm.t, data = dfs, importance = "impurity", write.forest = TRUE,      mtry = t.mrfX$bestTune$mtry, num.trees = 85, case.weights = case.weights.s) 

Type:                             Regression 
Number of trees:                  85 
Sample size:                      1130868 
Number of independent variables:  243 
Mtry:                             115 
Target node size:                 5 
Variable importance mode:         impurity 
Splitrule:                        variance 
OOB prediction error (MSE):       0.1268774 
R squared (OOB):                  0.9327077 

 Variable importance:
            [,1]
PC1   292386.575
PC242 183314.133
DEPTH 100067.411
PC218  92821.379
PC177  55980.656
PC224  42037.495
PC146  35904.045
PC219  33841.345
PC225  32854.876
PC226  32001.808
PC230  31472.787
PC117  29992.048
PC232  17331.664
PC241  15995.127
PC15   15852.303
PC165  15196.959
PC199  14147.158
PC113  13349.731
PC239  12365.672
PC4    12304.925
PC93   12197.684
PC240  11792.217
PC204  11590.057
PC28   11046.666
PC228   9882.183
PC161   8996.095
PC209   8856.354
PC150   8635.473
PC135   8199.807
PC234   8161.837
PC38    7833.506
PC12    7390.604
PC194   7179.557
PC231   7070.329
PC200   7067.871

eXtreme Gradient Boosting 

1130868 samples
    243 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 753912, 753912, 753912 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE       Rsquared   MAE      
  0.3  2           50      0.9514068  0.5203750  0.7488482
  0.3  2          100      0.9230369  0.5485493  0.7234762
  0.3  2          150      0.9076424  0.5635096  0.7098899
  0.3  3           50      0.9097325  0.5616002  0.7104643
  0.3  3          100      0.8793837  0.5902809  0.6835793
  0.3  3          150      0.8622113  0.6061422  0.6683680
  0.3  4           50      0.8768408  0.5928146  0.6802239
  0.3  4          100      0.8432191  0.6233827  0.6513869
  0.3  4          150      0.8218935  0.6422880  0.6334428
  0.4  2           50      0.9435959  0.5279720  0.7419241
  0.4  2          100      0.9159744  0.5552105  0.7170134
  0.4  2          150      0.8991237  0.5714505  0.7016664
  0.4  3           50      0.9012654  0.5693892  0.7017884
  0.4  3          100      0.8722069  0.5966708  0.6765286
  0.4  3          150      0.8536376  0.6137237  0.6606693
  0.4  4           50      0.8688714  0.5997658  0.6728681
  0.4  4          100      0.8346496  0.6306982  0.6437654
  0.4  4          150      0.8118278  0.6506978  0.6247413
  0.5  2           50      0.9401873  0.5312261  0.7370288
  0.5  2          100      0.9114647  0.5594243  0.7114144
  0.5  2          150      0.8943315  0.5758921  0.6965276
  0.5  3           50      0.8988820  0.5714887  0.6986518
  0.5  3          100      0.8692109  0.5993201  0.6732754
  0.5  3          150      0.8498693  0.6169612  0.6565699
  0.5  4           50      0.8637010  0.6043754  0.6681269
  0.5  4          100      0.8280080  0.6364293  0.6375400
  0.5  4          150      0.8042819  0.6570007  0.6179696

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'colsample_bytree'
 was held constant at a value of 0.8
Tuning parameter 'min_child_weight' was held constant at
 a value of 1
Tuning parameter 'subsample' was held constant at a value of 1
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nrounds = 150, max_depth = 4, eta = 0.5, gamma =
 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.

 XGBoost variable importance:
    Feature        Gain        Cover   Frequency
 1:     PC1 0.403847200 0.0265060535 0.026244344
 2:   PC113 0.038687966 0.0052936118 0.003619910
 3:   PC117 0.037909911 0.0055673959 0.004977376
 4:   PC218 0.023974096 0.0076209519 0.005882353
 5:   PC177 0.023331539 0.0022726622 0.003167421
 6:   PC219 0.022417324 0.0073143464 0.007239819
 7:    PC15 0.019350727 0.0219121395 0.014027149
 8:   PC150 0.015918428 0.0040639784 0.002714932
 9:   PC165 0.013994181 0.0043680092 0.003167421
10:   PC242 0.013295735 0.0086761704 0.009502262
11:     PC4 0.013292816 0.0129817408 0.014027149
12:   PC239 0.013285067 0.0176372020 0.010407240
13:   PC232 0.012528001 0.0104061251 0.009502262
14:    PC93 0.011462479 0.0118425512 0.007239819
15:   DEPTH 0.010231534 0.0232450182 0.027601810
16:   PC146 0.009631123 0.0067863929 0.002714932
17:    PC54 0.007803531 0.0040038062 0.004524887
18:   PC234 0.007638855 0.0101900578 0.008597285
19:   PC224 0.007610108 0.0042649938 0.005429864
20:   PC236 0.007597111 0.0060850666 0.003619910
21:   PC230 0.007208642 0.0055525150 0.006787330
22:    PC17 0.006833528 0.0047025814 0.005429864
23:   PC203 0.006828939 0.0024072080 0.002262443
24:     PC3 0.006618965 0.0077809353 0.008144796
25:    PC25 0.006339611 0.0005308627 0.003167421
    Feature        Gain        Cover   Frequency


 Support Vector Machine model:
SVM model on 243 features (cookie=1)
 Scenario: LS -1
 Formula:  ph_h2o ~ DEPTH + PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 +  
    PC9 + PC10 + PC11 + PC12 + PC13 + PC14 + PC15 + PC16 + PC17 +  
    PC18 + PC19 + PC20 + PC21 + PC22 + PC23 + PC24 + PC25 + PC26 +  
    PC27 + PC28 + PC29 + PC30 + PC31 + PC32 + PC33 + PC34 + PC35 +  
    PC36 + PC37 + PC38 + PC39 + PC40 + PC41 + PC42 + PC43 + PC44 +  
    PC45 + PC46 + PC47 + PC48 + PC49 + PC50 + PC51 + PC52 + PC53 +  
    PC54 + PC55 + PC56 + PC57 + PC58 + PC59 + PC60 + PC61 + PC62 +  
    PC63 + PC64 + PC65 + PC66 + PC67 + PC68 + PC69 + PC70 + PC71 +  
    PC72 + PC73 + PC74 + PC75 + PC76 + PC77 + PC78 + PC79 + PC80 +  
    PC81 + PC82 + PC83 + PC84 + PC85 + PC86 + PC87 + PC88 + PC89 +  
    PC90 + PC91 + PC92 + PC93 + PC94 + PC95 + PC96 + PC97 + PC98 +  
    PC99 + PC100 + PC101 + PC102 + PC103 + PC104 + PC105 + PC106 +  
    PC107 + PC108 + PC109 + PC110 + PC111 + PC112 + PC113 + PC114 +  
    PC115 + PC116 + PC117 + PC118 + PC119 + PC120 + PC121 + PC122 +  
    PC123 + PC124 + PC125 + PC126 + PC127 + PC128 + PC129 + PC130 +  
    PC131 + PC132 + PC133 + PC134 + PC135 + PC136 + PC137 + PC138 +  
    PC139 + PC140 + PC141 + PC142 + PC143 + PC144 + PC145 + PC146 +  
    PC147 + PC148 + PC149 + PC150 + PC151 + PC152 + PC153 + PC154 +  
    PC155 + PC156 + PC157 + PC158 + PC159 + PC160 + PC161 + PC162 +  
    PC163 + PC164 + PC165 + PC166 + PC167 + PC168 + PC169 + PC170 +  
    PC171 + PC172 + PC173 + PC174 + PC175 + PC176 + PC177 + PC178 +  
    PC179 + PC180 + PC181 + PC182 + PC183 + PC184 + PC185 + PC186 +  
    PC187 + PC188 + PC189 + PC190 + PC191 + PC192 + PC193 + PC194 +  
    PC195 + PC196 + PC197 + PC198 + PC199 + PC200 + PC201 + PC202 +  
    PC203 + PC204 + PC205 + PC206 + PC207 + PC208 + PC209 + PC210 +  
    PC211 + PC212 + PC213 + PC214 + PC215 + PC216 + PC217 + PC218 +  
    PC219 + PC220 + PC221 + PC222 + PC223 + PC224 + PC225 + PC226 +  
    PC227 + PC228 + PC229 + PC230 + PC231 + PC232 + PC233 + PC234 +  
    PC235 + PC236 + PC237 + PC238 + PC239 + PC240 + PC241 + PC242
  trained and selected on a 14x7047 grid
