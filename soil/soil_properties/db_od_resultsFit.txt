Results of model fitting 'randomForest and XGBoost':


Variable: bulkdens.fineearth
Ranger result

Call:
 ranger(formula = fm.t, data = dfs, importance = "impurity", write.forest = TRUE,      mtry = t.mrfX$bestTune$mtry, num.trees = 85, case.weights = case.weights.s) 

Type:                             Regression 
Number of trees:                  85 
Sample size:                      322854 
Number of independent variables:  243 
Mtry:                             115 
Target node size:                 5 
Variable importance mode:         impurity 
Splitrule:                        variance 
OOB prediction error (MSE):       0.01173787 
R squared (OOB):                  0.9113086 

 Variable importance:
           [,1]
PC1   5566.8839
DEPTH 3193.3320
PC27  2507.1225
PC212 1977.2128
PC29  1909.6728
PC125  942.1672
PC18   816.7938
PC30   815.0183
PC230  776.1990
PC32   763.0402
PC5    518.4956
PC225  506.4211
PC54   440.7842
PC241  435.1693
PC4    328.5120
PC205  306.0981
PC9    302.9143
PC137  286.5012
PC240  237.2998
PC106  232.8127
PC209  227.8884
PC234  211.7943
PC231  209.4683
PC40   177.0860
PC199  161.7075
PC227  159.4741
PC2    153.0677
PC17   149.7393
PC201  144.4245
PC184  142.9856
PC28   139.8961
PC49   139.0466
PC218  138.2735
PC166  133.7525
PC12   130.7110

eXtreme Gradient Boosting 

322854 samples
   243 predictor

No pre-processing
Resampling: Cross-Validated (3 fold, repeated 1 times) 
Summary of sample sizes: 215236, 215236, 215236 
Resampling results across tuning parameters:

  eta  max_depth  nrounds  RMSE       Rsquared   MAE      
  0.3  2           50      0.2368640  0.5776126  0.1709029
  0.3  2          100      0.2270609  0.6115750  0.1639956
  0.3  2          150      0.2207232  0.6330667  0.1595108
  0.3  3           50      0.2221561  0.6287700  0.1608622
  0.3  3          100      0.2106947  0.6660441  0.1527909
  0.3  3          150      0.2032087  0.6894222  0.1474686
  0.3  4           50      0.2094062  0.6703585  0.1517984
  0.3  4          100      0.1956837  0.7122123  0.1423451
  0.3  4          150      0.1858540  0.7404603  0.1352747
  0.4  2           50      0.2335334  0.5886540  0.1685514
  0.4  2          100      0.2234953  0.6233270  0.1615391
  0.4  2          150      0.2170089  0.6449235  0.1570486
  0.4  3           50      0.2187906  0.6390860  0.1585390
  0.4  3          100      0.2068245  0.6776183  0.1502205
  0.4  3          150      0.1983522  0.7035975  0.1442464
  0.4  4           50      0.2065621  0.6784693  0.1499081
  0.4  4          100      0.1911790  0.7247040  0.1392339
  0.4  4          150      0.1810554  0.7530644  0.1319036
  0.5  2           50      0.2319214  0.5938083  0.1677484
  0.5  2          100      0.2213562  0.6301546  0.1603667
  0.5  2          150      0.2148372  0.6516922  0.1558978
  0.5  3           50      0.2170864  0.6443423  0.1572389
  0.5  3          100      0.2041527  0.6855927  0.1483088
  0.5  3          150      0.1951709  0.7126160  0.1420726
  0.5  4           50      0.2029360  0.6892595  0.1474851
  0.5  4          100      0.1872504  0.7354768  0.1365515
  0.5  4          150      0.1769027  0.7639200  0.1288720

Tuning parameter 'gamma' was held constant at a value of 0
Tuning parameter 'colsample_bytree'
 was held constant at a value of 0.8
Tuning parameter 'min_child_weight' was held constant at
 a value of 1
Tuning parameter 'subsample' was held constant at a value of 1
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nrounds = 150, max_depth = 4, eta = 0.5, gamma =
 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1.

 XGBoost variable importance:
    Feature        Gain        Cover   Frequency
 1:     PC1 0.238848189 0.0130266481 0.030812325
 2:    PC29 0.107531683 0.0064293870 0.006535948
 3:   PC230 0.069134024 0.0065079010 0.004201681
 4:   DEPTH 0.057866916 0.0323209677 0.061157796
 5:   PC125 0.047593250 0.0056778739 0.004201681
 6:    PC27 0.021951652 0.0028404753 0.005602241
 7:    PC18 0.021566101 0.0049862130 0.007002801
 8:     PC4 0.018863854 0.0090518359 0.016339869
 9:   PC241 0.016800669 0.0157467428 0.010270775
10:    PC54 0.015008316 0.0047739995 0.003267974
11:   PC106 0.014307019 0.0036226940 0.006069094
12:     PC2 0.013356198 0.0028069820 0.017273576
13:   PC227 0.009439742 0.0031249926 0.004201681
14:   PC209 0.009176720 0.0069886802 0.004668534
15:    PC40 0.007838265 0.0074289764 0.007002801
16:   PC166 0.007562411 0.0049736479 0.002801120
17:   PC231 0.007297447 0.0046462057 0.003734827
18:   PC137 0.006828053 0.0067636589 0.004668534
19:   PC168 0.005933346 0.0086224736 0.007002801
20:    PC49 0.005848435 0.0050963412 0.003267974
21:   PC214 0.005836435 0.0005948773 0.001867414
22:    PC28 0.005763241 0.0068955826 0.008403361
23:     PC3 0.005398588 0.0112596972 0.014472456
24:    PC15 0.005321562 0.0025991565 0.006069094
25:   PC111 0.005034054 0.0020527897 0.004668534
    Feature        Gain        Cover   Frequency


 Support Vector Machine model:
SVM model on 243 features (cookie=1)
 Scenario: LS -1
 Formula:  db_od ~ DEPTH + PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 +  
    PC9 + PC10 + PC11 + PC12 + PC13 + PC14 + PC15 + PC16 + PC17 +  
    PC18 + PC19 + PC20 + PC21 + PC22 + PC23 + PC24 + PC25 + PC26 +  
    PC27 + PC28 + PC29 + PC30 + PC31 + PC32 + PC33 + PC34 + PC35 +  
    PC36 + PC37 + PC38 + PC39 + PC40 + PC41 + PC42 + PC43 + PC44 +  
    PC45 + PC46 + PC47 + PC48 + PC49 + PC50 + PC51 + PC52 + PC53 +  
    PC54 + PC55 + PC56 + PC57 + PC58 + PC59 + PC60 + PC61 + PC62 +  
    PC63 + PC64 + PC65 + PC66 + PC67 + PC68 + PC69 + PC70 + PC71 +  
    PC72 + PC73 + PC74 + PC75 + PC76 + PC77 + PC78 + PC79 + PC80 +  
    PC81 + PC82 + PC83 + PC84 + PC85 + PC86 + PC87 + PC88 + PC89 +  
    PC90 + PC91 + PC92 + PC93 + PC94 + PC95 + PC96 + PC97 + PC98 +  
    PC99 + PC100 + PC101 + PC102 + PC103 + PC104 + PC105 + PC106 +  
    PC107 + PC108 + PC109 + PC110 + PC111 + PC112 + PC113 + PC114 +  
    PC115 + PC116 + PC117 + PC118 + PC119 + PC120 + PC121 + PC122 +  
    PC123 + PC124 + PC125 + PC126 + PC127 + PC128 + PC129 + PC130 +  
    PC131 + PC132 + PC133 + PC134 + PC135 + PC136 + PC137 + PC138 +  
    PC139 + PC140 + PC141 + PC142 + PC143 + PC144 + PC145 + PC146 +  
    PC147 + PC148 + PC149 + PC150 + PC151 + PC152 + PC153 + PC154 +  
    PC155 + PC156 + PC157 + PC158 + PC159 + PC160 + PC161 + PC162 +  
    PC163 + PC164 + PC165 + PC166 + PC167 + PC168 + PC169 + PC170 +  
    PC171 + PC172 + PC173 + PC174 + PC175 + PC176 + PC177 + PC178 +  
    PC179 + PC180 + PC181 + PC182 + PC183 + PC184 + PC185 + PC186 +  
    PC187 + PC188 + PC189 + PC190 + PC191 + PC192 + PC193 + PC194 +  
    PC195 + PC196 + PC197 + PC198 + PC199 + PC200 + PC201 + PC202 +  
    PC203 + PC204 + PC205 + PC206 + PC207 + PC208 + PC209 + PC210 +  
    PC211 + PC212 + PC213 + PC214 + PC215 + PC216 + PC217 + PC218 +  
    PC219 + PC220 + PC221 + PC222 + PC223 + PC224 + PC225 + PC226 +  
    PC227 + PC228 + PC229 + PC230 + PC231 + PC232 + PC233 + PC234 +  
    PC235 + PC236 + PC237 + PC238 + PC239 + PC240 + PC241 + PC242
  trained and selected on a 13x2521 grid

--------------------------------------
